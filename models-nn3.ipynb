{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7212436,"sourceType":"datasetVersion","datasetId":4173449}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import ISRIStemmer\nfrom sklearn import preprocessing \nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\n#import libraries\nfrom tensorflow.keras.layers import InputLayer,Dense, Bidirectional, LSTM, Dropout, Activation, Embedding, Conv1D, GlobalMaxPooling1D,SimpleRNN\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences \n\n\n#Functions To Preprocess Dataset\ndef clean_reviews(text):\n    #remove_special_chars\n    pattern = re.compile(r'[^\\w\\s\\u0600-\\u06FF]+', re.UNICODE)\n    text = re.sub(pattern, '', text)\n    \n    #remove_num\n    text = re.sub(r'\\d+', '', text)\n\n    #remove_punc\n    text = re.sub(r'[^\\w\\s_]', '', text)\n    \n    #remove_non_arabic\n    pattern = re.compile(r'[^\\u0600-\\u06FF\\s]+', re.UNICODE)\n    text = re.sub(pattern, '', text)\n\n    #remove_repeating_char\n    text= re.sub(r'(.)\\1+', r'\\1', text)\n\n    \n    #remove_underscore\n    text=text.replace(\"_\", \"\")\n    #remove_stopwords\n    stop_words = set(stopwords.words('arabic'))\n    words = word_tokenize(text)\n    text = [word for word in words if word.lower() not in stop_words]\n    text_after_remove_stop_words=' '.join(text)\n    \n    #stemming\n    stemmer = ISRIStemmer()\n    words = word_tokenize(text_after_remove_stop_words)\n    stemmed_words = [stemmer.stem(word) for word in words]\n    return ' '.join(stemmed_words)\n     ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-25T08:50:13.411647Z","iopub.execute_input":"2023-12-25T08:50:13.412071Z","iopub.status.idle":"2023-12-25T08:50:30.766849Z","shell.execute_reply.started":"2023-12-25T08:50:13.412041Z","shell.execute_reply":"2023-12-25T08:50:30.765209Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Read Dataset","metadata":{}},{"cell_type":"code","source":"# read train_dataset\ntrain_dataset = pd.read_excel('/kaggle/input/nn-competition-files/train.xlsx') \n# clean the data\nreviews = train_dataset['review_description'].apply(clean_reviews)\nratings = train_dataset['rating']\n\nmax_fatures = 100# our model will remeber last 100 words\ntokenizer = Tokenizer (num_words=max_fatures, split=' ')\ntokenizer.fit_on_texts(reviews)\npad_train = tokenizer.texts_to_sequences (reviews)\npad_train= pad_sequences (pad_train) # padding to make all sentence at same length\n# encode ratings \ntrain_rating = ratings + 1 # Add 1 to convert -1->0 , 1->2 , 0->1 ","metadata":{"execution":{"iopub.status.busy":"2023-12-25T08:50:53.209033Z","iopub.execute_input":"2023-12-25T08:50:53.209822Z","iopub.status.idle":"2023-12-25T08:51:17.393977Z","shell.execute_reply.started":"2023-12-25T08:50:53.209783Z","shell.execute_reply":"2023-12-25T08:51:17.392324Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(pad_train, train_rating, test_size=0.3,stratify=train_rating)","metadata":{"execution":{"iopub.status.busy":"2023-12-25T08:51:25.874158Z","iopub.execute_input":"2023-12-25T08:51:25.874522Z","iopub.status.idle":"2023-12-25T08:51:25.899604Z","shell.execute_reply.started":"2023-12-25T08:51:25.874499Z","shell.execute_reply":"2023-12-25T08:51:25.898261Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#read test_dataset\ntest_dataset = pd.read_csv('/kaggle/input/nn-competition-files/test _no_label.csv') \n# clean the data\ncleaned_reviews = test_dataset['review_description'].apply(clean_reviews)\npad_test = tokenizer.texts_to_sequences (cleaned_reviews)\npad_test= pad_sequences (pad_test, maxlen=len(pad_train[0]))  # padding to make all sentence at same length","metadata":{"execution":{"iopub.status.busy":"2023-12-25T08:51:28.968384Z","iopub.execute_input":"2023-12-25T08:51:28.968720Z","iopub.status.idle":"2023-12-25T08:51:29.622032Z","shell.execute_reply.started":"2023-12-25T08:51:28.968691Z","shell.execute_reply":"2023-12-25T08:51:29.620572Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# CNN Model , Accuarcy = 0.75892","metadata":{}},{"cell_type":"code","source":"# CNN Model \nmax_features = 100  #our model will remember the last 100 words\nembed_dim = 4\nmodel_cnn = Sequential()\nmodel_cnn.add(Embedding(max_features, embed_dim, input_length=len(pad_train[0]), trainable=True))\nmodel_cnn.add(Conv1D(128, 5, activation='relu'))\nmodel_cnn.add(GlobalMaxPooling1D())\nmodel_cnn.add(Dense(3, activation='softmax'))\n\n#compile the model\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel_cnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_cnn.summary()\n#train the model\nmodel_cnn.fit(X_train, y_train, epochs=10, batch_size=32)\n\n#evaluate the model\nloss, accuracy = model_cnn.evaluate(X_test, y_test)\nprint('Model CNN loss = ', loss)\nprint('Model CNN accuracy = ', accuracy)\n\n#predict ratings using CNN \npredicted_ratings_cnn = model_cnn.predict(pad_test)\ny_new_pred_cnn = np.argmax(predicted_ratings_cnn, axis=1)\ny_new_pred_cnn = y_new_pred_cnn - 1\n\n#create a new data frame with the cleaned reviews and predicted ratings\nsubmission_csv_cnn = pd.DataFrame({'ID': range(1, 1001),'Predicted_Ratings': y_new_pred_cnn})\n#save the data frame to a CSV file\nsubmission_csv_cnn.to_csv('cnn.csv', index=False)  # Update with the desired filename and path\n","metadata":{"execution":{"iopub.status.busy":"2023-12-25T06:34:39.971740Z","iopub.execute_input":"2023-12-25T06:34:39.972351Z","iopub.status.idle":"2023-12-25T06:35:22.860660Z","shell.execute_reply.started":"2023-12-25T06:34:39.972301Z","shell.execute_reply":"2023-12-25T06:35:22.859582Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"Model: \"sequential\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_14 (Embedding)    (None, 92, 4)             400       \n                                                                 \n conv1d (Conv1D)             (None, 88, 128)           2688      \n                                                                 \n global_max_pooling1d (Glob  (None, 128)               0         \n alMaxPooling1D)                                                 \n                                                                 \n dense_27 (Dense)            (None, 3)                 387       \n                                                                 \n=================================================================\nTotal params: 3475 (13.57 KB)\nTrainable params: 3475 (13.57 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n701/701 [==============================] - 4s 4ms/step - loss: 0.5868 - accuracy: 0.7640\nEpoch 2/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5563 - accuracy: 0.7783\nEpoch 3/10\n701/701 [==============================] - 3s 5ms/step - loss: 0.5509 - accuracy: 0.7786\nEpoch 4/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5453 - accuracy: 0.7811\nEpoch 5/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5436 - accuracy: 0.7814\nEpoch 6/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5413 - accuracy: 0.7835\nEpoch 7/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5392 - accuracy: 0.7824\nEpoch 8/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5358 - accuracy: 0.7841\nEpoch 9/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5368 - accuracy: 0.7839\nEpoch 10/10\n701/701 [==============================] - 3s 4ms/step - loss: 0.5332 - accuracy: 0.7843\n301/301 [==============================] - 1s 2ms/step - loss: 0.5510 - accuracy: 0.7823\nModel CNN loss =  0.5510335564613342\nModel CNN accuracy =  0.782332718372345\n32/32 [==============================] - 0s 2ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# RNN Model , Accuarcy = 0.75","metadata":{}},{"cell_type":"code","source":"embed_dim = 4\nmodel_rnn = Sequential()\nmodel_rnn.add(Embedding(max_features, embed_dim, input_length=len(pad_train[0]), trainable=True))\nmodel_rnn.add(SimpleRNN(10, trainable=True))\nmodel_rnn.add(Dense(3, activation='softmax'))\n#compile the model\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel_rnn.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nmodel_rnn.summary()\n#train the model\nmodel_rnn.fit(X_train, y_train, epochs=10, batch_size=32)\n\n#evaluate the model\nloss, accuracy = model_rnn.evaluate(X_test, y_test)\nprint('Model RNN loss = ', loss)\nprint('Model RNN accuracy = ', accuracy)\n\n#predict ratings using RNN \npredicted_ratings_rnn = model_rnn.predict(pad_test)\n\ny_new_pred_rnn = np.argmax(predicted_ratings_rnn, axis=1)\ny_new_pred_rnn = y_new_pred_rnn - 1\n\n#create a new data frame with the cleaned reviews and predicted ratings\nsubmission_csv_rnn = pd.DataFrame({'ID': range(1, 1001),\n                                   'Predicted_Ratings': y_new_pred_rnn})\n\n#save the data frame to a CSV file\nsubmission_csv_rnn.to_csv('rnn.csv', index=False)  # Update with the desired filename and path\n","metadata":{"execution":{"iopub.status.busy":"2023-12-25T06:41:06.472295Z","iopub.execute_input":"2023-12-25T06:41:06.472663Z","iopub.status.idle":"2023-12-25T06:43:32.124304Z","shell.execute_reply.started":"2023-12-25T06:41:06.472634Z","shell.execute_reply":"2023-12-25T06:43:32.123430Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Model: \"sequential_1\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_16 (Embedding)    (None, 92, 4)             400       \n                                                                 \n simple_rnn (SimpleRNN)      (None, 10)                150       \n                                                                 \n dense_30 (Dense)            (None, 3)                 33        \n                                                                 \n=================================================================\nTotal params: 583 (2.28 KB)\nTrainable params: 583 (2.28 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n701/701 [==============================] - 14s 18ms/step - loss: 0.6259 - accuracy: 0.7484\nEpoch 2/10\n701/701 [==============================] - 12s 18ms/step - loss: 0.5697 - accuracy: 0.7722\nEpoch 3/10\n701/701 [==============================] - 13s 18ms/step - loss: 0.5647 - accuracy: 0.7754\nEpoch 4/10\n701/701 [==============================] - 12s 17ms/step - loss: 0.5806 - accuracy: 0.7690\nEpoch 5/10\n701/701 [==============================] - 12s 18ms/step - loss: 0.5965 - accuracy: 0.7651\nEpoch 6/10\n701/701 [==============================] - 13s 18ms/step - loss: 0.5661 - accuracy: 0.7741\nEpoch 7/10\n701/701 [==============================] - 12s 17ms/step - loss: 0.5608 - accuracy: 0.7762\nEpoch 8/10\n701/701 [==============================] - 13s 18ms/step - loss: 0.5592 - accuracy: 0.7772\nEpoch 9/10\n701/701 [==============================] - 12s 17ms/step - loss: 0.5549 - accuracy: 0.7808\nEpoch 10/10\n701/701 [==============================] - 12s 17ms/step - loss: 0.5595 - accuracy: 0.7787\n301/301 [==============================] - 2s 5ms/step - loss: 0.5720 - accuracy: 0.7708\nModel RNN loss =  0.5719732046127319\nModel RNN accuracy =  0.7707834839820862\n32/32 [==============================] - 0s 5ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Lstm Model , Accuarcy = 0.75595","metadata":{}},{"cell_type":"code","source":"embed_dim =4\nmax_fatures = 100\nmodel1 = Sequential()\nmodel1.add(Embedding(max_fatures, embed_dim, input_length = len(pad_train[0]),trainable=True))\nmodel1.add(LSTM(10,trainable=True))\nmodel1.add(Dense(3, activation='softmax'))\n# Compile the model\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel1.compile(loss='sparse_categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\nmodel1.summary()\nmodel1.fit(X_train,y_train, epochs=10, batch_size=32)\nloss,accuracy = model1.evaluate(X_test,y_test)\nprint('model1 loss = ',loss)\nprint('model1 accurcy = ',accuracy)\n\npredicted_ratings_model1=model1.predict(pad_test)\n# Convert predictions to class labels (-1, 0, 1)\nimport numpy as np\ny_new_pred_original=[]\ny_new_pred_original = np.argmax(predicted_ratings_model1, axis=1)\ny_new_pred_original=y_new_pred_original-1\n# Create a new DataFrame with the cleaned reviews and predicted ratings\nsubmtion_csv = pd.DataFrame({'ID':  range(1, 1001),\n                          'Predicted_Ratings': y_new_pred_original})\n\n# Save the DataFrame to a new CSV file\nsubmtion_csv.to_csv('/kaggle/working/sub_lstm.csv', index=False)  # Update with the desired filename and path","metadata":{"execution":{"iopub.status.busy":"2023-12-25T06:45:09.156044Z","iopub.execute_input":"2023-12-25T06:45:09.157852Z","iopub.status.idle":"2023-12-25T06:48:44.600150Z","shell.execute_reply.started":"2023-12-25T06:45:09.157814Z","shell.execute_reply":"2023-12-25T06:48:44.599325Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n embedding_17 (Embedding)    (None, 92, 4)             400       \n                                                                 \n lstm (LSTM)                 (None, 10)                600       \n                                                                 \n dense_31 (Dense)            (None, 3)                 33        \n                                                                 \n=================================================================\nTotal params: 1033 (4.04 KB)\nTrainable params: 1033 (4.04 KB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nEpoch 1/10\n701/701 [==============================] - 30s 28ms/step - loss: 0.5951 - accuracy: 0.7616\nEpoch 2/10\n701/701 [==============================] - 20s 29ms/step - loss: 0.5513 - accuracy: 0.7802\nEpoch 3/10\n701/701 [==============================] - 20s 28ms/step - loss: 0.5445 - accuracy: 0.7831\nEpoch 4/10\n701/701 [==============================] - 20s 29ms/step - loss: 0.5413 - accuracy: 0.7823\nEpoch 5/10\n701/701 [==============================] - 20s 29ms/step - loss: 0.5388 - accuracy: 0.7827\nEpoch 6/10\n701/701 [==============================] - 20s 28ms/step - loss: 0.5377 - accuracy: 0.7847\nEpoch 7/10\n701/701 [==============================] - 21s 29ms/step - loss: 0.5348 - accuracy: 0.7827\nEpoch 8/10\n701/701 [==============================] - 20s 29ms/step - loss: 0.5328 - accuracy: 0.7858\nEpoch 9/10\n701/701 [==============================] - 20s 28ms/step - loss: 0.5325 - accuracy: 0.7846\nEpoch 10/10\n701/701 [==============================] - 20s 29ms/step - loss: 0.5300 - accuracy: 0.7859\n301/301 [==============================] - 3s 7ms/step - loss: 0.5447 - accuracy: 0.7845\nmodel1 loss =  0.5447487235069275\nmodel1 accurcy =  0.784517765045166\n32/32 [==============================] - 1s 7ms/step\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GRU Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import GRU\nembed_dim = 4\nmax_features=100\nmodel_gru = Sequential()\nmodel_gru.add(Embedding(max_features, embed_dim, input_length=len(pad_train[0]), trainable=True))\nmodel_gru.add(GRU(128, activation='relu'))\nmodel_gru.add(Dense(3, activation='softmax'))\n# Compile the model\nopt = keras.optimizers.Adam(learning_rate=0.01)\nmodel_gru.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n# Train the model\nmodel_gru.fit(X_train, y_train, epochs=10, batch_size=32)\n# Evaluate the model\nloss_gru, accuracy_gru = model_gru.evaluate(X_test, y_test)\nprint('Model GRU loss = ', loss_gru)\nprint('Model GRU accuracy = ', accuracy_gru)\n# Predict ratings using GRU\npredicted_ratings_gru = model_gru.predict(pad_test)\n\ny_new_pred_gru = np.argmax(predicted_ratings_gru, axis=1)\ny_new_pred_gru = y_new_pred_gru - 1\n\n# Create a new data frame with the cleaned reviews and predicted ratings for GRU\nsubmission_csv_gru = pd.DataFrame({'ID': range(1, 1001),\n                                   'Predicted_Ratings': y_new_pred_gru})\n","metadata":{"execution":{"iopub.status.busy":"2023-12-25T08:52:32.089226Z","iopub.execute_input":"2023-12-25T08:52:32.089991Z","iopub.status.idle":"2023-12-25T09:01:02.580753Z","shell.execute_reply.started":"2023-12-25T08:52:32.089941Z","shell.execute_reply":"2023-12-25T09:01:02.578877Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch 1/10\n701/701 [==============================] - 48s 67ms/step - loss: 0.6854 - accuracy: 0.7575\nEpoch 2/10\n701/701 [==============================] - 45s 65ms/step - loss: 0.5469 - accuracy: 0.7860\nEpoch 3/10\n701/701 [==============================] - 45s 64ms/step - loss: 0.5425 - accuracy: 0.7847\nEpoch 4/10\n701/701 [==============================] - 46s 66ms/step - loss: 0.5394 - accuracy: 0.7843\nEpoch 5/10\n701/701 [==============================] - 45s 65ms/step - loss: 0.5383 - accuracy: 0.7859\nEpoch 6/10\n701/701 [==============================] - 46s 65ms/step - loss: 0.5349 - accuracy: 0.7876\nEpoch 7/10\n701/701 [==============================] - 45s 65ms/step - loss: 0.5339 - accuracy: 0.7872\nEpoch 8/10\n701/701 [==============================] - 48s 69ms/step - loss: 0.5326 - accuracy: 0.7877\nEpoch 9/10\n701/701 [==============================] - 46s 65ms/step - loss: 0.5315 - accuracy: 0.7868\nEpoch 10/10\n701/701 [==============================] - 46s 65ms/step - loss: 0.5304 - accuracy: 0.7871\n301/301 [==============================] - 6s 18ms/step - loss: 0.5519 - accuracy: 0.7780\nModel GRU loss =  0.5518734455108643\nModel GRU accuracy =  0.7779627442359924\n32/32 [==============================] - 1s 19ms/step\n","output_type":"stream"}]}]}